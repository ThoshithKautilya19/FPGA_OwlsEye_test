# FPGA-OwlsEye
[ALL CREDITS TO GAURAV SHAH AND HIS TEAM. THIS IS NOT MY REPOSITORY, I'VE USED THIS REPO TO UNDERSTAND, REPLICATE AND TWEAK A FEW THINGS DURING MY SUMMER PROJECT]
### (IEEE CASS Student Design Competition 2025 - India Winners)

FPGA-Owlseye is a real-time FPGA implementation of Video Object Detection designed to accomodate low-light conditions. It utilizes a custom Processing Logic (PL) for YOLOv5 FPGA acceleration that uses POSIT Computation, making it the **first POSIT FPGA implementation of complex ML models like YOLO.** This is acheived by modifying the [fpgaconvnet tool](https://fpgaconvnet.com/), specifically [fpgaconvnet-hls](https://github.com/AlexMontgomerie/fpgaconvnet-hls) and [fpgaconvnet-model](https://github.com/AlexMontgomerie/fpgaconvnet-model), using the [SoftPosit library](https://gitlab.com/cerlane/SoftPosit). To train the model parameters in POSIT format, we extend __PyTorch to EQTorch__ to enable training and fine-tuning or ML models (weights & activation) in any custom POSIT and Fixed-POSIT bit precision. FPGA-Owlseye is deployed on a Xilinx ZCU104 FPGA board for real-time testing, and the video for the entire project can be found on youtube here: 1. [without subtitles](https://youtu.be/jv4Sdfd4BVQ) 2. [with subtitles](https://youtu.be/f3KPzVUEX_4).

## EQTorch

[POSIT]("https://www.sigarch.org/posit-a-potential-replacement-for-ieee-754/") and [Fixed-POSIT]("https://ieeexplore.ieee.org/document/9399648") number systems were designed specifically to represent ML model parameters like weight and activations, so that they can offer replacement to the traditional Floating Point (IEEE754) by giving better energy efficiency, latency, and area size on hardware without compromising the ML model accuracy. This section explains how to use the CUDA and C kernels for Fixed Posit and Posit, so to extend the PyTorch framework and use it to train & fine-tune ML models in these format.

To install and use the EQTorch framework, navigate to the EQTorch directory and install the framework using the following command:

```bash
pip install -e ./
```
Once installed, you can import the tools from `qtorch` as follows:

```python
from qtorch.quant import Quantizer, quantizer
from qtorch.optim import OptimLP
from qtorch import Posit, FixedPosit
```
Refer to the example folder within the EQTorch directory for sample usage.

## Fixed-POSIT and POSIT Multipliers 

The repository also contains the synthesizable verilog `POSIT` and `Fixed-POSIT` multipliers. They can be used to perform comparative study of performance parameters like latency, power, and area against traditional Floating Point and Integer Multipliers with tools like cadence genus. Multiplier codes can be found [here](./mulitpliers/).

## Fpgaconvnet

The [modified fpgaconvnet](./fpgaconvnet/) contains the tool that can be used to deploy **any** regular and irregular ML on a target FPGA platform with the use of POSIT & Fixed-POSIT computing. The repository contains [fpgaconvnet-tutorial](./fpgaconvnet/tutorial/) sub-directory which can be explored to understand the entire toolflow from model & fpga specification -> Automated PL IP generation -> Vivado Block Design -> PS Code in Vitis IDE -> Live Inference with host code. 

## Live Scripts & Results

The [deploy](./deploy/) folder contains the scripts used to deploy the owlseye low-light detection pipeline on the ZCU104 FPGA platform. We use the [Pynq framework](https://www.pynq.io/) for interfacing with the FPGA board and programming the `Processing System (PS)` which is a quad-core ARM® Cortex™-A53 processor. To optimize the output throughput (FPS), we utilize asynchronous execution of the enhancement and detection model, by executing INT8 qunatized & Pruned zerodce enhancement network on the `PS` and YOLOv5 detection network on custom `PL` generated by fpgaconvnet (uses POSIT computation).  

The videos showing the owlseye performance are in the `./results` folder.
1. [Captured low-light video](./results/captured.avi)
2. [Enhancement](./results/enhanced.avi)
3. [Enahncement + Detection](./results/enhanced_detected.avi)

## NOTE
This Repository also containes files in [./test](./test) to run the Owsleye low-light detection pipeline on a Intel Embedded Platform using Openvino. This can be useful for testing and prototyping the enhancement and detection models before their deployment to the FPGA the [jupyter notebook](./test/yolov8-instance-segmentation.ipynb) used for downloading, quantizing by QAT, and verifying the accuracy for the yolov8-nano instance segmentation model that it used in the real-time code for live video instance segmentation.

To understand how the quantized model is being used in the final runfile, You can refer to [live_optimized.py](./test/live_optimized.py) file.

## References:

[1] Gaurav Shah, A. Goud, Z. Momin, and J. Mekie, "OwlsEye: Real-Time Low-Light Video Instance Segmentation on Edge and Exploration of Fixed-Posit Quantization," presented at the 2025 38th International Conference on VLSI Design and 2025 24th International Conference on Embedded Systems (VLSID), Bengaluru, India, Jan. 4–8, 2025. [Online]. Available: https://drive.google.com/file/d/1RhpU7zS5hsCJNjFDx7BJfd-3m71EHuEA/view?usp=sharing.


